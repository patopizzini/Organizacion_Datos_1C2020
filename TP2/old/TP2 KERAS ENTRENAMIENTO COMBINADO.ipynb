{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75.06 - 1C202 - TP2 \n",
    "# Competencia de Machine learning\n",
    "#\n",
    "# https://www.kaggle.com/c/nlp-getting-started/overview\n",
    "#\n",
    "# Fuentes:\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "# https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n",
    "\n",
    "# Importacion de librerias necesarias\n",
    "import re, string, random, datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split y K-Fold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Importacion de Keras\n",
    "# https://keras.io\n",
    "from keras.models import Model, Sequential, save_model, load_model\n",
    "from keras.layers import Embedding, Conv1D, Dropout, Input, GlobalMaxPooling1D, Dense, concatenate, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Importación para MLP.\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import glob\n",
    "#import cv2\n",
    "import os\n",
    "\n",
    "#Configuracion general\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seccion de CONFIGURACION\n",
    "\n",
    "#Configuración de la limpieza de datos\n",
    "#Pasaje a minúsculas:   True\n",
    "#Limpieza básica:       True\n",
    "#Agregado de keywords:  False\n",
    "#Remoción de stopwords: False\n",
    "#Lematización:          False\n",
    "#Aumento del set:       False\n",
    "#Chequeo final:         True\n",
    "\n",
    "# Path de entrada    \n",
    "train_path = 'data/processed/train.2020.08.03T16.57.04.289197.csv' #Cambiar por el archivo pre-procesado a usar!\n",
    "test_path = 'data/processed/test.2020.08.03T16.57.04.289197.csv' #Cambiar por el archivo pre-procesado a usar!\n",
    "\n",
    "# Configuracion del modelo\n",
    "maxlen = 100\n",
    "embedding_dim = 300\n",
    "embeddings_path = 'embeddings/glove.42B.300d.txt'\n",
    "# Embeddings disponibles en:\n",
    "# https://www.kaggle.com/yutanakamura/glove42b300dtxt\n",
    "\n",
    "# Configuracion de entrenamiento\n",
    "epochs = 15\n",
    "verbose = False\n",
    "batch_size = 16\n",
    "\n",
    "# Canditad de folds para K-Fold Cross-Validation\n",
    "num_folds = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def procesar_categoricas_numericas(df, train, test):\n",
    "    # Definimos las columnas continuas.\n",
    "    continuous = [\"length\", \"totalwords\"]\n",
    "    # Normalizamos las columnas a un rango de [0, 1]\n",
    "    cs = MinMaxScaler()\n",
    "    trainContinuous = cs.fit_transform(train[continuous])\n",
    "    testContinuous = cs.transform(test[continuous])\n",
    "    # Aplicamos un one-hot encoding de las columans para dejarlas en un rango [0, 1])\n",
    "    zipBinarizer = LabelBinarizer().fit(df[\"keyword\"])\n",
    "    trainKeywordCategorical = zipBinarizer.transform(train[\"keyword\"])\n",
    "    testKeywordCategorical = zipBinarizer.transform(test[\"keyword\"])\n",
    "    # Concatenamos las columnas categóricas y las numéricas a usar.\n",
    "    trainX = np.hstack([trainKeywordCategorical, trainContinuous])\n",
    "    testX = np.hstack([testKeywordCategorical, testContinuous])\n",
    "    # retornamos nuestro set de datos de entrenamiento y prueba.\n",
    "    return (trainX, testX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim, regress=False):\n",
    "    # Defeninimos la red perceptron.\n",
    "    model = Sequential()\n",
    "    model.add(Dense(8, input_dim=dim, activation=\"relu\"))\n",
    "    model.add(Dense(4, activation=\"relu\"))\n",
    "    if regress:\n",
    "        model.add(Dense(1, activation=\"linear\"))\n",
    "    # retornamos el modelo.\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones AUXILIARES\n",
    "# Grafica para un modelo los resultados del entrenamiento\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Entrenamiento')\n",
    "    plt.plot(x, val_acc, 'r', label='Validación')\n",
    "    plt.title('Accuracy: validación y entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Entrenamiento')\n",
    "    plt.plot(x, val_loss, 'r', label='Validación')\n",
    "    plt.title('Loss: validación y entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Genera la matriz de embeddings\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # 0 es un indice reservado, sumamos 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (7613, 7)\n",
      "Shape test: (3263, 6)\n"
     ]
    }
   ],
   "source": [
    "# Carga de los archivos del set de datos\n",
    "# Set de train: carga\n",
    "df_twitter_train = pd.read_csv(train_path, sep=',')\n",
    "# Set de test: carga\n",
    "df_twitter_test = pd.read_csv(test_path, sep=',')\n",
    "\n",
    "# Print de los shapes\n",
    "print('Shape train: ' + str(df_twitter_train.shape))\n",
    "print('Shape test: ' + str(df_twitter_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cobertura de vocabulario: 0.77944649669957\n"
     ]
    }
   ],
   "source": [
    "# Nos quedamos con X e y para entrenar\n",
    "tweets = df_twitter_train['text'].values\n",
    "y = df_twitter_train['target'].values\n",
    "\n",
    "# Separamos X para el set de test\n",
    "tweets_predict = df_twitter_test['text'].values\n",
    "\n",
    "# Tokenizamos los textos\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# Convertimos a secuencias\n",
    "X = tokenizer.texts_to_sequences(tweets)\n",
    "X_predict = tokenizer.texts_to_sequences(tweets_predict)\n",
    "\n",
    "# Calculamos el tamaño\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 0 es un indice reservado, sumamos 1\n",
    "\n",
    "# Paddeamos a maxlen\n",
    "X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "X_predict = pad_sequences(X_predict, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Generacion de la matriz de embeddings\n",
    "embedding_matrix = create_embedding_matrix(embeddings_path,tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Vemos que cobertura tenemos con los embeddings utilizados\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print('Cobertura de vocabulario: ' + str(nonzero_elements / vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#===================================================================================================\n",
    "# ARMAMOS EL MODELO PARA CON PERCEPTRON MULTI CAPAS.\n",
    "#==================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainAttrX, testAttrX) = train_test_split(df_twitter_train, test_size=0.25, random_state=42)\n",
    "(trainAttrX, testAttrX) = procesar_categoricas_numericas(df_twitter_train,trainAttrX, testAttrX)\n",
    "mlp = create_mlp(trainAttrX.shape[1], regress=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the input to our final set of layers as the *output* of boththe MLP and CNN\n",
    "# combinedInput = concatenate([mlp.output, loaded_model.output])\n",
    "\n",
    "# our final FC layer head will have two dense layers, the final one\n",
    "# being our regression head\n",
    "# x = Dense(4, activation=\"relu\")(combinedInput)\n",
    "# x = Dense(1, activation=\"linear\")(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#==================================================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total de folds:  2\n",
      "------------------------------------------------------------------------\n",
      "Entrenando, fold 1...\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"dense_input:0\", shape=(None, 224), dtype=float32) at layer \"dense_input\". The following previous layers were accessed without issue: ['input_3', 'embedding_2']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-52a15cb39e63>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0;31m# Definimos la arquitectura del modelo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtweet_input\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmlp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m     model.compile(loss='binary_crossentropy',\n\u001b[1;32m     47\u001b[0m                   \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'adam'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/OrgaDatos/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    165\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0m_keras_api_gauge\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_cell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'model'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m     \u001b[0;31m# Model must be created under scope of DistStrat it will be trained with.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/OrgaDatos/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    171\u001b[0m         'inputs' in kwargs and 'outputs' in kwargs):\n\u001b[1;32m    172\u001b[0m       \u001b[0;31m# Graph network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 173\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_graph_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    174\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m       \u001b[0;31m# Subclassed network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/OrgaDatos/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    454\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_self_setattr_tracking\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprevious_value\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/OrgaDatos/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_init_graph_network\u001b[0;34m(self, inputs, outputs, name, **kwargs)\u001b[0m\n\u001b[1;32m    304\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    305\u001b[0m     \u001b[0;31m# Keep track of the network's nodes and layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 306\u001b[0;31m     nodes, nodes_by_depth, layers, _ = _map_graph_network(\n\u001b[0m\u001b[1;32m    307\u001b[0m         self.inputs, self.outputs)\n\u001b[1;32m    308\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_network_nodes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/OrgaDatos/lib/python3.8/site-packages/tensorflow/python/keras/engine/network.py\u001b[0m in \u001b[0;36m_map_graph_network\u001b[0;34m(inputs, outputs)\u001b[0m\n\u001b[1;32m   1785\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_tensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1787\u001b[0;31m             raise ValueError('Graph disconnected: '\n\u001b[0m\u001b[1;32m   1788\u001b[0m                              \u001b[0;34m'cannot obtain value for tensor '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1789\u001b[0m                              \u001b[0;34m' at layer \"'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'\". '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"dense_input:0\", shape=(None, 224), dtype=float32) at layer \"dense_input\". The following previous layers were accessed without issue: ['input_3', 'embedding_2']"
     ]
    }
   ],
   "source": [
    "# Definimos y entrenamos el modelo\n",
    "# Define el modelo a utilizar, con Conv1D\n",
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=maxlen, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=maxlen, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=maxlen, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "\n",
    "merged = concatenate([mlp.output, bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "# Path de para guardar los modelos\n",
    "model_path = 'models/TP2.Keras.Conv1D.Iter.'\n",
    "\n",
    "# Definimos los arrays para guardar los resultados\n",
    "# Train\n",
    "acc_per_fold_train = []\n",
    "loss_per_fold_train = []\n",
    "# Validacion\n",
    "acc_per_fold_validation = []\n",
    "loss_per_fold_validation = []\n",
    "\n",
    "# Definomos el K-fold Cross Validator a usar\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Hacemos el bucle para la cantidad de folds definidos\n",
    "fold_no = 1\n",
    "print('Total de folds: ', num_folds)\n",
    "for train, validation in kfold.split(X, y):\n",
    "\n",
    "    # Imprimimos el progreso\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Entrenando, fold {fold_no}...')\n",
    "\n",
    "    # Definimos la arquitectura del modelo\n",
    "    model = Model(inputs=[tweet_input], outputs=[mlp.output,output])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Hacemos el fit\n",
    "    history = model.fit(X[train], y[train],\n",
    "                        epochs = epochs,\n",
    "                        verbose = verbose,\n",
    "                        validation_data = (X[validation], y[validation]),\n",
    "                        batch_size = batch_size)      \n",
    "\n",
    "    # Generamos la metrica de entrenamiento\n",
    "    scores_train = model.evaluate(X[train], y[train], verbose=0)\n",
    "    print(f'Puntaje de entrenamiento para el fold {fold_no}: {model.metrics_names[0]} de {scores_train[0]}; {model.metrics_names[1]} de {scores_train[1]*100}%')\n",
    "    acc_per_fold_train.append(scores_train[1] * 100)\n",
    "    loss_per_fold_train.append(scores_train[0])\n",
    "\n",
    "    # Generamos la metrica de test\n",
    "    scores_validation = model.evaluate(X[validation], y[validation], verbose=0)\n",
    "    print(f'Puntaje de validación para el fold {fold_no}: {model.metrics_names[0]} de {scores_validation[0]}; {model.metrics_names[1]} de {scores_validation[1]*100}%')\n",
    "    acc_per_fold_validation.append(scores_validation[1] * 100)\n",
    "    loss_per_fold_validation.append(scores_validation[0])\n",
    "\n",
    "    # Guardamos el modelo\n",
    "    save_model(model, model_path + str(fold_no) + '.h5', save_format='h5')\n",
    "\n",
    "    # Plotteamos el resultado final\n",
    "    plot_history(history)\n",
    "\n",
    "    # Incrementamos el fold\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "print('\\n')\n",
    "print('Puntaje promedio de entrenamiendo, para todos los folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_train)} (+- {np.std(acc_per_fold_train)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold_train)}')\n",
    "print('\\n')\n",
    "print('Puntaje promedio de validación, para todos los folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_validation)} (+- {np.std(acc_per_fold_validation)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold_validation)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de salida para el submission\n",
    "submission_path = 'data/submits/submission.' + datetime.datetime.now().isoformat() + '.csv'\n",
    "submission_path = submission_path.replace('-','.').replace(':','.')\n",
    "\n",
    "# Cargamos el modelo con el mejor puntaje\n",
    "filepath = model_path + str((acc_per_fold_validation.index(max(acc_per_fold_validation)))+1) + '.h5'\n",
    "loaded_model = load_model(filepath, custom_objects=None, compile=True)\n",
    "\n",
    "# Prediccion en base al modelo entrenado\n",
    "y_predict = (loaded_model.predict(X_predict) > 0.5).astype('int32')\n",
    "\n",
    "# Generacion del dataframe que generara el submit para la competencia\n",
    "kaggle_submission = pd.DataFrame(df_twitter_test,columns = ['id'])\n",
    "kaggle_submission['target'] = y_predict\n",
    "\n",
    "kaggle_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "# Imprimimos un resumen de la operacion\n",
    "print('Operación finalizada!\\n')\n",
    "print('Generado submit: \\'' + submission_path + '\\' - (' + str(len(kaggle_submission['target'].index)) + ') registros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo repetimos con todo el dataset\n",
    "# Creamos y entrenamos nuevamente\n",
    "model2 = create_model(num_filters = num_filters,\n",
    "                         kernel_size = kernel_size,\n",
    "                         vocab_size = vocab_size,\n",
    "                         embedding_dim = embedding_dim,\n",
    "                         maxlen = maxlen)\n",
    "\n",
    "# Hacemos el fit\n",
    "history = model2.fit(X, y,\n",
    "                        epochs = epochs,\n",
    "                        verbose = verbose,\n",
    "                        batch_size = batch_size)\n",
    "\n",
    "# Prediccion en base al modelo entrenado\n",
    "y_predict = (model2.predict(X_predict) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de salida para el submission\n",
    "submission_path = 'data/submits/submission.' + datetime.datetime.now().isoformat() + '.csv'\n",
    "submission_path = submission_path.replace('-','.').replace(':','.')\n",
    "\n",
    "# Generacion del dataframe que generara el submit para la competencia\n",
    "kaggle_submission = pd.DataFrame(df_twitter_test,columns = ['id'])\n",
    "kaggle_submission['target'] = y_predict\n",
    "\n",
    "kaggle_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "# Imprimimos un resumen de la operacion\n",
    "print('Operación finalizada!\\n')\n",
    "print('Generado submit: \\'' + submission_path + '\\' - (' + str(len(kaggle_submission['target'].index)) + ') registros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
