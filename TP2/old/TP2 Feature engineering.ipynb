{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTACIÓN GENERAL DE LIBRERIAS.\n",
    "import re \n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import geocoder\n",
    "import requests\n",
    "import warnings\n",
    "import descartes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import datetime as DT\n",
    "#import geopandas as gpd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SKLEARN.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# RANDOM FOREST.\n",
    "from urllib.request import urlopen\n",
    "#from shapely.geometry import Point, Polygon\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# XGBOOST.\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK: https://www.nltk.org\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#WORDCLOUD\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# CONFIGURACIÓN.\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\") \n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importacion del archivo CSV de fuente\n",
    "#https://www.kaggle.com/c/nlp-getting-started\n",
    "original_train = pd.read_csv('data/train.csv')\n",
    "original_test = pd.read_csv('data/test.csv')\n",
    "original_sample_submission = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASAMOS TODO A MINÚSCULAS (TRAIN).\n",
    "original_train['text'] = original_train['text'].str.lower()\n",
    "original_train['location'] = original_train['location'].str.lower()\n",
    "original_train['keyword'] = original_train['keyword'].str.lower()\n",
    "#PASAMOS TODO A MINÚSCULAS (TEST).\n",
    "original_test['text'] = original_test['text'].str.lower()\n",
    "original_test['location'] = original_test['location'].str.lower()\n",
    "original_test['keyword'] = original_test['keyword'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LONGITUD DEL TWEET Y CANTIDAD DE PALABRAS (TRAIN).\n",
    "original_train['length'] = original_train['text'].str.len()\n",
    "original_train['totalwords'] = original_train['text'].str.split().str.len()\n",
    "original_train['words'] = original_train.text.str.strip().str.split()\n",
    "#LONGITUD DEL TWEET Y CANTIDAD DE PALABRAS (TEST).\n",
    "original_test['length'] = original_test['text'].str.len()\n",
    "original_test['totalwords'] = original_test['text'].str.split().str.len()\n",
    "original_test['words'] = original_test.text.str.strip().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'filtrarPalabras' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-c09ef402ec32>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TRAIN).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moriginal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfiltrarPalabras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TEST).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moriginal_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfiltrarPalabras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-c09ef402ec32>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TRAIN).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0moriginal_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfiltrarPalabras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TEST).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0moriginal_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moriginal_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhashtags\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfiltrarPalabras\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mel\u001b[0m \u001b[0;32min\u001b[0m \u001b[0moriginal_test\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'filtrarPalabras' is not defined"
     ]
    }
   ],
   "source": [
    "#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TRAIN).\n",
    "original_train = original_train.assign(hashtags=[filtrarPalabras(el) for el in original_train.words])\n",
    "#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TEST).\n",
    "original_test = original_test.assign(hashtags=[filtrarPalabras(el) for el in original_test.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTAMOS LA CANTIDAD DE HASHTAGS (TRAIN).\n",
    "original_train['hashtagsCantidad'] = original_train['hashtags'].str.count('#')\n",
    "original_train['preguntas'] = original_train['text'].str.count('[!]') + original_train['text'].str.count('[?]')\n",
    "original_train['simbolos'] = original_train['text'].str.count('[!]') + original_train['text'].str.count('[?]') + original_train['text'].str.count('=') + original_train['text'].str.count('>')\n",
    "#CONTAMOS LA CANTIDAD DE HASHTAGS (TEST).\n",
    "original_test['hashtagsCantidad'] = original_test['hashtags'].str.count('#')\n",
    "original_test['preguntas'] = original_test['text'].str.count('[!]') + original_test['text'].str.count('[?]')\n",
    "original_test['simbolos'] = original_test['text'].str.count('[!]') + original_test['text'].str.count('[?]') + original_test['text'].str.count('=') + original_test['text'].str.count('>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>totalwords</th>\n",
       "      <th>words</th>\n",
       "      <th>hashtags</th>\n",
       "      <th>hashtagsCantidad</th>\n",
       "      <th>preguntas</th>\n",
       "      <th>simbolos</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>[our, deeds, are, the, reason, of, this, #eart...</td>\n",
       "      <td>#earthquake</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>[forest, fire, near, la, ronge, sask., canada]</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "      <td>[all, residents, asked, to, 'shelter, in, plac...</td>\n",
       "      <td></td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>[13,000, people, receive, #wildfires, evacuati...</td>\n",
       "      <td>#wildfires</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>[just, got, sent, this, photo, from, ruby, #al...</td>\n",
       "      <td>#alaska #wildfires</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  our deeds are the reason of this #earthquake m...   \n",
       "1   4     NaN      NaN             forest fire near la ronge sask. canada   \n",
       "2   5     NaN      NaN  all residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  just got sent this photo from ruby #alaska as ...   \n",
       "\n",
       "   target  length  totalwords  \\\n",
       "0       1      69          13   \n",
       "1       1      38           7   \n",
       "2       1     133          22   \n",
       "3       1      65           8   \n",
       "4       1      88          16   \n",
       "\n",
       "                                               words             hashtags  \\\n",
       "0  [our, deeds, are, the, reason, of, this, #eart...          #earthquake   \n",
       "1     [forest, fire, near, la, ronge, sask., canada]                        \n",
       "2  [all, residents, asked, to, 'shelter, in, plac...                        \n",
       "3  [13,000, people, receive, #wildfires, evacuati...           #wildfires   \n",
       "4  [just, got, sent, this, photo, from, ruby, #al...   #alaska #wildfires   \n",
       "\n",
       "   hashtagsCantidad  preguntas  simbolos  \n",
       "0                 1          0         0  \n",
       "1                 0          0         0  \n",
       "2                 0          0         0  \n",
       "3                 1          0         0  \n",
       "4                 2          0         0  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MARCAMOS QUIENES USAN UBICACIÓN Y PALABRAS CLAVES (TRAIN).\n",
    "original_train['conUbicacion'] = 0\n",
    "original_train['conKeyword'] = 0\n",
    "original_train.loc[original_train['location'] != 'vacio', ['conUbicacion']] = 1\n",
    "original_train.loc[original_train['keyword'] != 'vacio', ['conKeyword']] = 1\n",
    "#MARCAMOS QUIENES USAN UBICACIÓN Y PALABRAS CLAVES (TEST).\n",
    "original_test['conUbicacion'] = 0\n",
    "original_test['conKeyword'] = 0\n",
    "original_test.loc[original_test['location'] != 'vacio', ['conUbicacion']] = 1\n",
    "original_test.loc[original_test['keyword'] != 'vacio', ['conKeyword']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOT ENCODING PARA KEYWORD (TRAIN).\n",
    "dummies = pd.get_dummies(original_train['keyword'], drop_first=False)\n",
    "original_train = pd.concat([original_train, dummies], axis=1)\n",
    "original_train.drop('keyword', 1, inplace = True)\n",
    "#HOT ENCODING PARA KEYWORD (TEST).\n",
    "dummies = pd.get_dummies(original_test['keyword'], drop_first=False)\n",
    "original_test = pd.concat([original_test, dummies], axis=1)\n",
    "original_test.drop('keyword', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELIMINAMOS LAS COLUMNAS QUE NO SON NUMÉRICAS (TRAIN).\n",
    "original_train.drop('words', 1, inplace = True)\n",
    "original_train.drop('location', 1, inplace = True)\n",
    "original_train.drop('hashtags', 1, inplace = True)\n",
    "#ELIMINAMOS LAS COLUMNAS QUE NO SON NUMÉRICAS (TEST).\n",
    "original_test.drop('words', 1, inplace = True)\n",
    "original_test.drop('location', 1, inplace = True)\n",
    "original_test.drop('hashtags', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>totalwords</th>\n",
       "      <th>hashtagsCantidad</th>\n",
       "      <th>preguntas</th>\n",
       "      <th>simbolos</th>\n",
       "      <th>conUbicacion</th>\n",
       "      <th>conKeyword</th>\n",
       "      <th>...</th>\n",
       "      <th>weapons</th>\n",
       "      <th>whirlwind</th>\n",
       "      <th>wild%20fires</th>\n",
       "      <th>wildfire</th>\n",
       "      <th>windstorm</th>\n",
       "      <th>wounded</th>\n",
       "      <th>wounds</th>\n",
       "      <th>wreck</th>\n",
       "      <th>wreckage</th>\n",
       "      <th>wrecked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>our deeds are the reason of this #earthquake m...</td>\n",
       "      <td>1</td>\n",
       "      <td>69</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>forest fire near la ronge sask. canada</td>\n",
       "      <td>1</td>\n",
       "      <td>38</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>all residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "      <td>133</td>\n",
       "      <td>22</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>just got sent this photo from ruby #alaska as ...</td>\n",
       "      <td>1</td>\n",
       "      <td>88</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 231 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                               text  target  length  \\\n",
       "0   1  our deeds are the reason of this #earthquake m...       1      69   \n",
       "1   4             forest fire near la ronge sask. canada       1      38   \n",
       "2   5  all residents asked to 'shelter in place' are ...       1     133   \n",
       "3   6  13,000 people receive #wildfires evacuation or...       1      65   \n",
       "4   7  just got sent this photo from ruby #alaska as ...       1      88   \n",
       "\n",
       "   totalwords  hashtagsCantidad  preguntas  simbolos  conUbicacion  \\\n",
       "0          13                 1          0         0             1   \n",
       "1           7                 0          0         0             1   \n",
       "2          22                 0          0         0             1   \n",
       "3           8                 1          0         0             1   \n",
       "4          16                 2          0         0             1   \n",
       "\n",
       "   conKeyword  ...  weapons  whirlwind  wild%20fires  wildfire  windstorm  \\\n",
       "0           1  ...        0          0             0         0          0   \n",
       "1           1  ...        0          0             0         0          0   \n",
       "2           1  ...        0          0             0         0          0   \n",
       "3           1  ...        0          0             0         0          0   \n",
       "4           1  ...        0          0             0         0          0   \n",
       "\n",
       "   wounded  wounds  wreck  wreckage  wrecked  \n",
       "0        0       0      0         0        0  \n",
       "1        0       0      0         0        0  \n",
       "2        0       0      0         0        0  \n",
       "3        0       0      0         0        0  \n",
       "4        0       0      0         0        0  \n",
       "\n",
       "[5 rows x 231 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train.to_csv('data/processed/train_to_keras.csv',index=False)\n",
    "original_test.to_csv('data/processed/test_to_keras.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
