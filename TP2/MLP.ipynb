{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importacion de librerias necesarias\n",
    "import re, string, random, datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpia los datos, mediante el uso de expresiones regulares\n",
    "def f_remove_noise(text):\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de entrada\n",
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (7613, 5)\n",
      "Shape test: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "# Carga de los archivos del set de datos\n",
    "# Set de train: carga\n",
    "df_twitter_train = pd.read_csv(train_path, sep=',')\n",
    "# Set de test: carga\n",
    "df_twitter_test = pd.read_csv(test_path, sep=',')\n",
    "\n",
    "# Print de los shapes\n",
    "print('Shape train: ' + str(df_twitter_train.shape))\n",
    "print('Shape test: ' + str(df_twitter_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_train['text'] = df_twitter_train['text'].apply(lambda x: f_remove_noise(x))\n",
    "df_twitter_test['text'] = df_twitter_test['text'].apply(lambda x: f_remove_noise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Disaster Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>debris</th>\n",
       "      <td>37</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wreckage</th>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>derailment</th>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outbreak</th>\n",
       "      <td>40</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil%20spill</th>\n",
       "      <td>38</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typhoon</th>\n",
       "      <td>38</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide%20bombing</th>\n",
       "      <td>33</td>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide%20bomber</th>\n",
       "      <td>31</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bombing</th>\n",
       "      <td>29</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescuers</th>\n",
       "      <td>35</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Count  Disaster Probability\n",
       "keyword                                       \n",
       "debris                37              1.000000\n",
       "wreckage              39              1.000000\n",
       "derailment            39              1.000000\n",
       "outbreak              40              0.975000\n",
       "oil%20spill           38              0.973684\n",
       "typhoon               38              0.973684\n",
       "suicide%20bombing     33              0.969697\n",
       "suicide%20bomber      31              0.967742\n",
       "bombing               29              0.931034\n",
       "rescuers              35              0.914286"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_stats = df_twitter_train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n",
    "keywords_disaster = keyword_stats.loc[keyword_stats['Disaster Probability']==1]\n",
    "keywords_no_disaster  = keyword_stats.loc[keyword_stats['Disaster Probability']==0]\n",
    "keyword_stats.sort_values('Disaster Probability', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#keywords_disaster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels , val_labels = train_test_split(\n",
    "    df_twitter_train['text'].values, df_twitter_train[\"target\"].values, test_size=0.15, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "                             min_df=2,      # only use words that appear at least X times\n",
    "                             #stop_words='english', # remove stop words\n",
    "                             #lowercase=True, # Convert everything to lower case \n",
    "                             use_idf=True,   # Use idf\n",
    "                             norm=u'l2',     # Normalization\n",
    "                             smooth_idf=True, # Prevents divide-by-zero errors\n",
    "                             ngram_range=(1,3),\n",
    "                             #dtype='int32',\n",
    "                             analyzer='word',\n",
    "                             strip_accents = 'unicode',\n",
    "                             decode_error = 'replace'\n",
    "                            )\n",
    "x_train = vectorizer.fit_transform(train_texts)\n",
    "x_val = vectorizer.transform(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=min(10000, x_train.shape[1]))\n",
    "selector.fit(x_train, train_labels)\n",
    "x_train = selector.transform(x_train)\n",
    "x_val = selector.transform(x_val)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "learning_rate=1e-4\n",
    "epochs=1000\n",
    "batch_size=128\n",
    "layers=2\n",
    "units=64\n",
    "dropout_rate=0.2\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Dropout(rate=dropout_rate, input_shape=x_train.shape[1:]))\n",
    "\n",
    "for _ in range(layers-1):\n",
    "    model.add(Dense(units=units, activation='relu'))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation accuracy: 0.8099824786186218, loss: 0.44409993290901184\n"
     ]
    }
   ],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "# Create callback for early stopping on validation loss. If the loss does\n",
    "# not decrease in two consecutive tries, stop training.\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=5)]\n",
    "\n",
    "# Train and validate model.\n",
    "history = model.fit(\n",
    "        x_train.toarray(),\n",
    "        train_labels,\n",
    "        epochs=epochs,\n",
    "        callbacks=callbacks,\n",
    "        validation_data=(x_val.toarray(), val_labels),\n",
    "        verbose=0,  # Logs once per epoch.\n",
    "        batch_size=batch_size)\n",
    "\n",
    "# Print results.\n",
    "history = history.history\n",
    "print('Validation accuracy: {acc}, loss: {loss}'.format(\n",
    "        acc=history['val_acc'][-1], loss=history['val_loss'][-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**************************************************\n",
      " MLP Model f1_score: 0.41429\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "y = df_twitter_train[\"target\"].values\n",
    "\n",
    "x_all = vectorizer.transform(df_twitter_train['text'].values)\n",
    "x_all = selector.transform(x_all)\n",
    "y_predict = model.predict_classes(x_all.toarray())\n",
    "\n",
    "score = f1_score(df_twitter_train[\"target\"].values, y_predict, average='weighted')\n",
    "print(\"*\"*50+\"\\n MLP Model f1_score: {:.5f}\\n\".format(score)+\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_test = pd.read_csv('../data/test.csv')\n",
    "original_sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "test_all = vectorizer.transform(original_test['text'].values)\n",
    "test_all = selector.transform(test_all)\n",
    "\n",
    "y_predict = model.predict_classes(test_all.toarray())\n",
    "y_predict[original_test.loc[original_test['keyword'].isin(list(keywords_disaster.index) )].index]=1\n",
    "y_predict[original_test.loc[original_test['keyword'].isin(list(keywords_no_disaster.index) )].index]=0\n",
    "\n",
    "original_sample_submission[\"target\"] = y_predict\n",
    "original_sample_submission.to_csv(\"data/submits/submission_MLP_08.csv\", index=False)\n",
    "original_sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN = pd.read_csv('data/submits/submission.2020.08.03T21.22.36.100631.csv')\n",
    "XGBOOST = pd.read_csv('data/submits/submission_XGB_12.csv')\n",
    "MLP = pd.read_csv('data/submits/submission_MLP_08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.8109102053325161\n",
    "ediccion =(CNN[\"target\"]+XGBOOST[\"target\"]+MLP[\"target\"])/3\n",
    "y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.8026356114005516\n",
    "# ediccion =(CNN[\"target\"]+MLP[\"target\"])/2\n",
    "# y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.7716825007661661\n",
    "# ediccion =(XGBOOST[\"target\"]+MLP[\"target\"])/2\n",
    "# y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.7805700275819798\n",
    "# ediccion =(CNN[\"target\"]+XGBOOST[\"target\"])/2\n",
    "# y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    2440\n",
       "1     823\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "original_test = pd.read_csv('data/test.csv')\n",
    "kaggle_submission = pd.DataFrame(original_test,columns = ['id'])\n",
    "kaggle_submission[\"target\"] = y_pred_ENS\n",
    "kaggle_submission[\"target\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "kaggle_submission.to_csv(\"data/submits/submission_ENSAMBLE_CNN_MLP_XGB_08.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
