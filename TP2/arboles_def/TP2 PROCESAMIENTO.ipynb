{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## IMPORTACIÓN GENERAL DE LIBRERIAS.\n",
    "import re \n",
    "import sys\n",
    "import json\n",
    "import nltk\n",
    "import geocoder\n",
    "import requests\n",
    "import warnings\n",
    "import descartes\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import networkx as nx\n",
    "import datetime as DT\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# SKLEARN.\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# RANDOM FOREST.\n",
    "from urllib.request import urlopen\n",
    "from shapely.geometry import Point, Polygon\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# XGBOOST.\n",
    "from xgboost import XGBClassifier\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK: https://www.nltk.org\n",
    "from langdetect import detect\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "#WORDCLOUD\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# CONFIGURACIÓN.\n",
    "%matplotlib inline\n",
    "plt.style.use('default')\n",
    "pd.options.display.float_format = '{:20,.2f}'.format\n",
    "warnings.filterwarnings('ignore')\n",
    "sns.set(style=\"whitegrid\") \n",
    "plt.rcParams['figure.figsize'] = (15, 10)\n",
    "np.set_printoptions(threshold=sys.maxsize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import requests\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from string import punctuation\n",
    "from textblob import TextBlob\n",
    "\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize, regexp_tokenize # funtions for standard tokenisation\n",
    "from nltk.tokenize import TweetTokenizer # function for tweets tokenization\n",
    "from nltk.corpus import stopwords\n",
    "import string\n",
    "\n",
    "from nltk.util import bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Limpia puntuacion, quita también usuarios y hashtags (@ y #)\n",
    "def clean_tweet(words):\n",
    "    result = []\n",
    "    for word in words:\n",
    "        stripped_word = word.strip()\n",
    "        if ((stripped_word.isalnum() == True) and (not(stripped_word.isdigit()))):      \n",
    "            result.append(stripped_word)\n",
    "    \n",
    "    return result\n",
    "\n",
    "def filtrarPalabras(miArray):\n",
    "    variable = ''\n",
    "    for key in miArray:\n",
    "         if '#' in str(key):\n",
    "            variable = variable + ' ' + str(key)\n",
    "    return variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tknzr = TweetTokenizer() # initialization of Tweet Tokenizer\n",
    "\n",
    "def mean_words_length(text):\n",
    "    words = word_tokenize(text)\n",
    "    word_lengths = [len(w) for w in words]\n",
    "    return round(np.mean(word_lengths),1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean(tweet): \n",
    "           \n",
    "    # Urls\n",
    "    tweet = re.sub(r\"https?:\\/\\/t.co\\/[A-Za-z0-9]+\", \"\", tweet)\n",
    "        \n",
    "    # Words with punctuations and special characters\n",
    "    punctuations = '@#!?+&*[]-%.:/();$=><|{}^' + \"'`\"\n",
    "    for p in punctuations:\n",
    "        tweet = tweet.replace(p, f' {p} ')\n",
    "        \n",
    "    # ... and ..\n",
    "    tweet = tweet.replace('...', ' ... ')\n",
    "    if '...' not in tweet:\n",
    "        tweet = tweet.replace('..', ' ... ') \n",
    "    \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =====================================================================\n",
    "### LECTURA DE CSV.\n",
    "### ====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importacion del archivo CSV de fuente\n",
    "#https://www.kaggle.com/c/nlp-getting-started\n",
    "original_train = pd.read_csv('../data/train.csv')\n",
    "original_test = pd.read_csv('../data/test.csv')\n",
    "original_sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =====================================================================\n",
    "### PROCESAMIENTO DE DATOS.\n",
    "### ====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#COMPLETAMOS LOS NULOS CON UN TEXTO: 'VACIO'\n",
    "original_train.fillna('vacio', inplace = True)\n",
    "original_test.fillna('vacio', inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train['text'] = original_train['text'].apply(lambda s : clean(s))\n",
    "original_test['text'] = original_test['text'].apply(lambda s : clean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASAMOS TODO A MINÚSCULAS (TRAIN).\n",
    "original_train['text'] = original_train['text'].str.lower()\n",
    "original_train['location'] = original_train['location'].str.lower()\n",
    "original_train['keyword'] = original_train['keyword'].str.lower()\n",
    "#PASAMOS TODO A MINÚSCULAS (TEST).\n",
    "original_test['text'] = original_test['text'].str.lower()\n",
    "original_test['location'] = original_test['location'].str.lower()\n",
    "original_test['keyword'] = original_test['keyword'].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==================================================\n",
    "## TRATAMIENTO INCLUYENDO STOP WORDS.\n",
    "## =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LONGITUD DEL TWEET Y CANTIDAD DE PALABRAS (TRAIN).\n",
    "original_train['length'] = original_train['text'].str.len()\n",
    "original_train['totalwords'] = original_train['text'].str.split().str.len()\n",
    "original_train['words'] = original_train.text.str.strip().str.split()\n",
    "#LONGITUD DEL TWEET Y CANTIDAD DE PALABRAS (TEST).\n",
    "original_test['length'] = original_test['text'].str.len()\n",
    "original_test['totalwords'] = original_test['text'].str.split().str.len()\n",
    "original_test['words'] = original_test.text.str.strip().str.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ARMANDO UNA LISTA DE PALABRAS QUE PUEDEN REPRESENTAR CATÁSTROFES (TRAIN & TEST).\n",
    "selected_words_singular=['fire','flood','inundate','earthquake','quake','deluge','euption','twister','tornado','hurricane', 'landslide','typhoon','wildfire','forest fire','drought','avalanche','urgent','important','danger','warrning','evacuation']\n",
    "selected_words_plural=['fires','floods', 'earthquakes','quakes','deluges','rashes','tornadoes','hurricanes', 'landslides','typhoons','wildfires','forest fires','droughts','avalanches']\n",
    "selected_words_other=['heat wave','died','flooding','flooded','damage','urgent','important','danger','warrning','help','evacuation']\n",
    "col_one_list = original_train['keyword'].tolist()\n",
    "selected_words = selected_words_singular + selected_words_plural + selected_words_other + col_one_list\n",
    "s = set(selected_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TRAIN).\n",
    "original_train = original_train.assign(hashtags=[filtrarPalabras(el) for el in original_train.words])\n",
    "original_train = original_train.assign(matches=[len(set(el) & s) for el in original_train.words])\n",
    "#BUSCAMOS LAS COINCIDENCIAS CON PALABRAS CLAVES Y SEPARAMOS LOS HASHTAGS (TEST).\n",
    "original_test = original_test.assign(hashtags=[filtrarPalabras(el) for el in original_test.words])\n",
    "original_test = original_test.assign(matches=[len(set(el) & s) for el in original_test.words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#CONTAMOS LA CANTIDAD DE HASHTAGS (TRAIN).\n",
    "original_train['preguntas'] = original_train['text'].str.count('[!]') + original_train['text'].str.count('[?]')\n",
    "original_train['simbolos'] = original_train['text'].str.count('[!]') + original_train['text'].str.count('[?]') + original_train['text'].str.count('=') + original_train['text'].str.count('>')\n",
    "#CONTAMOS LA CANTIDAD DE HASHTAGS (TEST).\n",
    "original_test['preguntas'] = original_test['text'].str.count('[!]') + original_test['text'].str.count('[?]')\n",
    "original_test['simbolos'] = original_test['text'].str.count('[!]') + original_test['text'].str.count('[?]') + original_test['text'].str.count('=') + original_test['text'].str.count('>')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MARCAMOS QUIENES USAN UBICACIÓN Y PALABRAS CLAVES (TRAIN).\n",
    "original_train['conUbicacion'] = 0\n",
    "original_train['conKeyword'] = 0\n",
    "original_train.loc[original_train['location'] != 'vacio', ['conUbicacion']] = 1\n",
    "original_train.loc[original_train['keyword'] != 'vacio', ['conKeyword']] = 1\n",
    "#MARCAMOS QUIENES USAN UBICACIÓN Y PALABRAS CLAVES (TEST).\n",
    "original_test['conUbicacion'] = 0\n",
    "original_test['conKeyword'] = 0\n",
    "original_test.loc[original_test['location'] != 'vacio', ['conUbicacion']] = 1\n",
    "original_test.loc[original_test['keyword'] != 'vacio', ['conKeyword']] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# original_train['text'] = original_train['text'].apply(lambda s : clean(s))\n",
    "# original_test['text'] = original_test['text'].apply(lambda s : clean(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==================================================\n",
    "## TRATAMIENTO QUITANDO LAS STOP WORDS.\n",
    "## =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamiento del set con nltk y nlpaug\n",
    "# NLTK\n",
    "# Sabiendo que todos los tweets son en idioma ingles, quitamos las stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "original_train['text'] =  original_train['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))\n",
    "original_test['text'] =  original_test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#PASAMOS A LA RAÍZ IDOMÁTICA DE LA PALABRA.\n",
    "stemmer = SnowballStemmer('english')\n",
    "original_train['text'] = original_train['text'].apply(lambda x:' '.join([stemmer.stem(y) for y in x.split()]))\n",
    "original_test['text'] = original_test['text'].apply(lambda x:' '.join([stemmer.stem(y) for y in x.split()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LONGITUD DEL TWEET Y CANTIDAD DE PALABRAS (TRAIN).\n",
    "original_train['length_SW'] = original_train['text'].str.len()\n",
    "original_train['totalwords_SW'] = original_train['text'].str.split().str.len()\n",
    "original_train['lenXword_SW'] = (original_train['length_SW'] / original_train['totalwords_SW'])\n",
    "original_train['stopWords'] = (original_train['totalwords'] - original_train['totalwords_SW']) + 1\n",
    "original_train['totalXsw'] = (original_train['totalwords_SW'] / original_train['stopWords'])\n",
    "#LONGITUD DEL TWEET Y CANTIDAD DE PALABRAS (TEST).\n",
    "original_test['length_SW'] = original_test['text'].str.len()\n",
    "original_test['totalwords_SW'] = original_test['text'].str.split().str.len()\n",
    "original_test['lenXword_SW'] = (original_test['length_SW'] / original_test['totalwords_SW'])\n",
    "original_test['stopWords'] = (original_test['totalwords'] - original_test['totalwords_SW']) + 1\n",
    "original_test['totalXsw'] = (original_test['totalwords_SW'] / original_test['stopWords'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numbers_regex = r\"(\\d+\\.?,?\\s?\\d+)\"\n",
    "hashtags_regex = r\"#\\w+\"\n",
    "mentions_regex = r\"@\\w+\"\n",
    "punctuation_regex = r\"[^\\w\\s]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words count\n",
    "original_train['words_count'] = original_train['text'].apply(lambda x: len(tknzr.tokenize(x)))\n",
    "# numbers count\n",
    "original_train['numbers_count'] = original_train['text'].apply(lambda x: len(regexp_tokenize(x, numbers_regex)))\n",
    "# hashtags count\n",
    "original_train['hashtags_count'] = original_train['text'].apply(lambda x: len(regexp_tokenize(x, hashtags_regex)))\n",
    "# mentions count\n",
    "original_train['mentions_count'] = original_train['text'].apply(lambda x: len(regexp_tokenize(x, mentions_regex)))\n",
    "# mean words length\n",
    "original_train['mean_words_length'] = original_train['text'].apply(mean_words_length)\n",
    "# mean words length\n",
    "original_train['characters_count'] = original_train['text'].apply(lambda x: len(x))\n",
    "# lowercase tokens\n",
    "original_train['lowercase_bag_o_w'] = original_train['text'].apply(lambda x: [w for w in tknzr.tokenize(x.lower())])\n",
    "# stopwords\n",
    "original_train['stopwords'] = original_train['lowercase_bag_o_w'].apply(lambda x: [t for t in x if t in stopwords.words('english')])\n",
    "# stopwords count\n",
    "original_train['stopwords_count'] = original_train['stopwords'].apply(lambda x: len(x))\n",
    "# alpha words only (excludes mentions and hashtags)\n",
    "original_train['alpha_only'] = original_train['lowercase_bag_o_w'].apply(lambda x: [t for t in x if t.isalpha()])\n",
    "# counts of alpha words only\n",
    "original_train['alpha_count'] = original_train['alpha_only'].apply(lambda x: len(x))\n",
    "# counts of punctuation marks only\n",
    "original_train['punctuation_count'] = original_train['text'].apply(lambda x: len(regexp_tokenize(x, punctuation_regex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# words count\n",
    "original_test['words_count'] = original_test['text'].apply(lambda x: len(tknzr.tokenize(x)))\n",
    "# numbers count\n",
    "original_test['numbers_count'] = original_test['text'].apply(lambda x: len(regexp_tokenize(x, numbers_regex)))\n",
    "# hashtags count\n",
    "original_test['hashtags_count'] = original_test['text'].apply(lambda x: len(regexp_tokenize(x, hashtags_regex)))\n",
    "# mentions count\n",
    "original_test['mentions_count'] = original_test['text'].apply(lambda x: len(regexp_tokenize(x, mentions_regex)))\n",
    "# mean words length\n",
    "original_test['mean_words_length'] = original_test['text'].apply(mean_words_length)\n",
    "# mean words length\n",
    "original_test['characters_count'] = original_test['text'].apply(lambda x: len(x))\n",
    "# lowercase tokens\n",
    "original_test['lowercase_bag_o_w'] = original_test['text'].apply(lambda x: [w for w in tknzr.tokenize(x.lower())])\n",
    "# stopwords\n",
    "original_test['stopwords'] = original_test['lowercase_bag_o_w'].apply(lambda x: [t for t in x if t in stopwords.words('english')])\n",
    "# stopwords count\n",
    "original_test['stopwords_count'] = original_test['stopwords'].apply(lambda x: len(x))\n",
    "# alpha words only (excludes mentions and hashtags)\n",
    "original_test['alpha_only'] = original_test['lowercase_bag_o_w'].apply(lambda x: [t for t in x if t.isalpha()])\n",
    "# counts of alpha words only\n",
    "original_test['alpha_count'] = original_test['alpha_only'].apply(lambda x: len(x))\n",
    "# counts of punctuation marks only\n",
    "original_test['punctuation_count'] = original_test['text'].apply(lambda x: len(regexp_tokenize(x, punctuation_regex)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#QUITAMOS LOS NÚMEROS.\n",
    "original_train['text'] = original_train['text'].str.replace('\\d+', '')\n",
    "original_test['text'] = original_test['text'].str.replace('\\d+', '')\n",
    "original_train['text'] = original_train['text'].str.replace('vacio', '')\n",
    "original_test['text'] = original_test['text'].str.replace('vacio', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==================================================\n",
    "## ENCODING\n",
    "## =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOT ENCODING PARA KEYWORD (TRAIN).\n",
    "dummies = pd.get_dummies(original_train['keyword'], drop_first=False)\n",
    "original_train = pd.concat([original_train, dummies], axis=1)\n",
    "original_train.drop('keyword', 1, inplace = True)\n",
    "\n",
    "#HOT ENCODING PARA KEYWORD (TEST).\n",
    "dummies = pd.get_dummies(original_test['keyword'], drop_first=False)\n",
    "original_test = pd.concat([original_test, dummies], axis=1)\n",
    "original_test.drop('keyword', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = ce.BinaryEncoder(cols=['keyword'])\n",
    "# original_train = encoder.fit_transform(original_train)\n",
    "# original_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder = ce.BinaryEncoder(cols=['keyword'])\n",
    "# original_test = encoder.fit_transform(original_test)\n",
    "# original_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train = original_train.fillna(0)\n",
    "original_test = original_test.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.corpus import twitter_samples, stopwords\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import FreqDist, classify, NaiveBayesClassifier\n",
    "import nltk\n",
    "import re, string, random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_noise(tweet_tokens, stop_words = ()):\n",
    "\n",
    "    cleaned_tokens = []\n",
    "\n",
    "    for token, tag in pos_tag(tweet_tokens):\n",
    "        token = re.sub('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+#]|[!*\\(\\),]|'\\\n",
    "                       '(?:%[0-9a-fA-F][0-9a-fA-F]))+','', token)\n",
    "        token = re.sub(\"(@[A-Za-z0-9_]+)\",\"\", token)\n",
    "\n",
    "        if tag.startswith(\"NN\"):\n",
    "            pos = 'n'\n",
    "        elif tag.startswith('VB'):\n",
    "            pos = 'v'\n",
    "        else:\n",
    "            pos = 'a'\n",
    "\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        token = lemmatizer.lemmatize(token, pos)\n",
    "\n",
    "        if len(token) > 0 and token not in string.punctuation and token.lower() not in stop_words:\n",
    "            cleaned_tokens.append(token.lower())\n",
    "    return cleaned_tokens\n",
    "\n",
    "def get_all_words(cleaned_tokens_list):\n",
    "    for tokens in cleaned_tokens_list:\n",
    "        for token in tokens:\n",
    "            yield token\n",
    "\n",
    "def get_tweets_for_model(cleaned_tokens_list):\n",
    "    for tweet_tokens in cleaned_tokens_list:\n",
    "        yield dict([token, True] for token in tweet_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/andres/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/andres/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package twitter_samples to\n",
      "[nltk_data]     /home/andres/nltk_data...\n",
      "[nltk_data]   Package twitter_samples is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/andres/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /home/andres/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('twitter_samples')\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(':)', 3691), (':-)', 701), (':d', 658), ('thanks', 388), ('follow', 357), ('love', 333), ('...', 290), ('good', 283), ('get', 263), ('thank', 253)]\n"
     ]
    }
   ],
   "source": [
    "    positive_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "    negative_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "    text = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "    tweet_tokens = twitter_samples.tokenized('positive_tweets.json')[0]\n",
    "\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    positive_tweet_tokens = twitter_samples.tokenized('positive_tweets.json')\n",
    "    negative_tweet_tokens = twitter_samples.tokenized('negative_tweets.json')\n",
    "\n",
    "    positive_cleaned_tokens_list = []\n",
    "    negative_cleaned_tokens_list = []\n",
    "\n",
    "    for tokens in positive_tweet_tokens:\n",
    "        positive_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    for tokens in negative_tweet_tokens:\n",
    "        negative_cleaned_tokens_list.append(remove_noise(tokens, stop_words))\n",
    "\n",
    "    all_pos_words = get_all_words(positive_cleaned_tokens_list)\n",
    "\n",
    "    freq_dist_pos = FreqDist(all_pos_words)\n",
    "    print(freq_dist_pos.most_common(10))\n",
    "\n",
    "    positive_tokens_for_model = get_tweets_for_model(positive_cleaned_tokens_list)\n",
    "    negative_tokens_for_model = get_tweets_for_model(negative_cleaned_tokens_list)\n",
    "\n",
    "    positive_dataset = [(tweet_dict, \"Positive\")\n",
    "                         for tweet_dict in positive_tokens_for_model]\n",
    "\n",
    "    negative_dataset = [(tweet_dict, \"Negative\")\n",
    "                         for tweet_dict in negative_tokens_for_model]\n",
    "\n",
    "    dataset = positive_dataset + negative_dataset\n",
    "\n",
    "    random.shuffle(dataset)\n",
    "\n",
    "    train_data = dataset[:7000]\n",
    "    test_data = dataset[7000:]\n",
    "\n",
    "    classifier = NaiveBayesClassifier.train(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train['token_limpios'] = original_train['text'].apply(lambda s : remove_noise(word_tokenize(s)))\n",
    "original_train['sentimiento'] = original_train['token_limpios'].apply(lambda s: classifier.classify(dict([token, True] for token in s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test['token_limpios'] = original_test['text'].apply(lambda s : remove_noise(word_tokenize(s)))\n",
    "original_test['sentimiento'] = original_test['token_limpios'].apply(lambda s: classifier.classify(dict([token, True] for token in s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HOT ENCODING PARA SENTIMIENTO (TRAIN).\n",
    "dummies = pd.get_dummies(original_train['sentimiento'], drop_first=False)\n",
    "original_train = pd.concat([original_train, dummies], axis=1)\n",
    "original_train.drop('sentimiento', 1, inplace = True)\n",
    "original_train.drop('token_limpios', 1, inplace = True)\n",
    "\n",
    "#HOT ENCODING PARA SENTIMIENTO (TEST).\n",
    "dummies = pd.get_dummies(original_test['sentimiento'], drop_first=False)\n",
    "original_test = pd.concat([original_test, dummies], axis=1)\n",
    "original_test.drop('sentimiento', 1, inplace = True)\n",
    "original_test.drop('token_limpios', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ==================================================\n",
    "## DEJAMOS SOLO LAS COLUMNAS NUMÉRICAS.\n",
    "## =================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ELIMINAMOS LAS COLUMNAS QUE NO SON NUMÉRICAS (TRAIN).\n",
    "original_train.drop('text', 1, inplace = True)\n",
    "original_train.drop('words', 1, inplace = True)\n",
    "original_train.drop('location', 1, inplace = True)\n",
    "original_train.drop('hashtags', 1, inplace = True)\n",
    "#ELIMINAMOS LAS COLUMNAS QUE NO SON NUMÉRICAS (TEST).\n",
    "original_test.drop('text', 1, inplace = True)\n",
    "original_test.drop('words', 1, inplace = True)\n",
    "original_test.drop('location', 1, inplace = True)\n",
    "original_test.drop('hashtags', 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train.drop(['lowercase_bag_o_w','stopwords','alpha_only'], axis=1, inplace= True)\n",
    "original_test.drop(['lowercase_bag_o_w','stopwords','alpha_only'], axis=1, inplace= True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =====================================================================\n",
    "### RESGUARDAMOS LOS DATOS EN CSV.\n",
    "### ====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>length</th>\n",
       "      <th>totalwords</th>\n",
       "      <th>matches</th>\n",
       "      <th>preguntas</th>\n",
       "      <th>simbolos</th>\n",
       "      <th>conUbicacion</th>\n",
       "      <th>conKeyword</th>\n",
       "      <th>length_SW</th>\n",
       "      <th>...</th>\n",
       "      <th>wild%20fires</th>\n",
       "      <th>wildfire</th>\n",
       "      <th>windstorm</th>\n",
       "      <th>wounded</th>\n",
       "      <th>wounds</th>\n",
       "      <th>wreck</th>\n",
       "      <th>wreckage</th>\n",
       "      <th>wrecked</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>71</td>\n",
       "      <td>14</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>40</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>38</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>139</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>75</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>67</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>92</td>\n",
       "      <td>18</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 247 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  length  totalwords  matches  preguntas  simbolos  conUbicacion  \\\n",
       "0   1       1      71          14        1          0         0             0   \n",
       "1   4       1      40           8        1          0         0             0   \n",
       "2   5       1     139          25        1          0         0             0   \n",
       "3   6       1      67           9        2          0         0             0   \n",
       "4   7       1      92          18        2          0         0             0   \n",
       "\n",
       "   conKeyword  length_SW  ...  wild%20fires  wildfire  windstorm  wounded  \\\n",
       "0           0         43  ...             0         0          0        0   \n",
       "1           0         38  ...             0         0          0        0   \n",
       "2           0         75  ...             0         0          0        0   \n",
       "3           0         52  ...             0         0          0        0   \n",
       "4           0         56  ...             0         0          0        0   \n",
       "\n",
       "   wounds  wreck  wreckage  wrecked  Negative  Positive  \n",
       "0       0      0         0        0         0         1  \n",
       "1       0      0         0        0         0         1  \n",
       "2       0      0         0        0         0         1  \n",
       "3       0      0         0        0         1         0  \n",
       "4       0      0         0        0         1         0  \n",
       "\n",
       "[5 rows x 247 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vemos la estructura del dataframe TRAIN.\n",
    "original_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Columns: 247 entries, id to Positive\n",
      "dtypes: float64(3), int64(20), uint8(224)\n",
      "memory usage: 3.0 MB\n"
     ]
    }
   ],
   "source": [
    "original_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>length</th>\n",
       "      <th>totalwords</th>\n",
       "      <th>matches</th>\n",
       "      <th>preguntas</th>\n",
       "      <th>simbolos</th>\n",
       "      <th>conUbicacion</th>\n",
       "      <th>conKeyword</th>\n",
       "      <th>length_SW</th>\n",
       "      <th>totalwords_SW</th>\n",
       "      <th>...</th>\n",
       "      <th>wild%20fires</th>\n",
       "      <th>wildfire</th>\n",
       "      <th>windstorm</th>\n",
       "      <th>wounded</th>\n",
       "      <th>wounds</th>\n",
       "      <th>wreck</th>\n",
       "      <th>wreckage</th>\n",
       "      <th>wrecked</th>\n",
       "      <th>Negative</th>\n",
       "      <th>Positive</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>24</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>68</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>9</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>96</td>\n",
       "      <td>19</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>59</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>36</td>\n",
       "      <td>7</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>45</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>37</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 246 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  length  totalwords  matches  preguntas  simbolos  conUbicacion  \\\n",
       "0   0      34           6        1          0         0             0   \n",
       "1   2      68          11        1          0         0             0   \n",
       "2   3      96          19        1          0         0             0   \n",
       "3   9      46           7        2          0         0             0   \n",
       "4  11      45           8        1          0         0             0   \n",
       "\n",
       "   conKeyword  length_SW  totalwords_SW  ...  wild%20fires  wildfire  \\\n",
       "0           0         24              4  ...             0         0   \n",
       "1           0         52              9  ...             0         0   \n",
       "2           0         59             10  ...             0         0   \n",
       "3           0         36              7  ...             0         0   \n",
       "4           0         37              6  ...             0         0   \n",
       "\n",
       "   windstorm  wounded  wounds  wreck  wreckage  wrecked  Negative  Positive  \n",
       "0          0        0       0      0         0        0         1         0  \n",
       "1          0        0       0      0         0        0         0         1  \n",
       "2          0        0       0      0         0        0         1         0  \n",
       "3          0        0       0      0         0        0         0         1  \n",
       "4          0        0       0      0         0        0         1         0  \n",
       "\n",
       "[5 rows x 246 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vemos la estructura del dataframe TEST.\n",
    "original_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3263 entries, 0 to 3262\n",
      "Columns: 246 entries, id to Positive\n",
      "dtypes: float64(3), int64(19), uint8(224)\n",
      "memory usage: 1.2 MB\n"
     ]
    }
   ],
   "source": [
    "original_test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target\n",
       "0   0       0\n",
       "1   2       0\n",
       "2   3       0\n",
       "3   9       0\n",
       "4  11       0"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Vemos la estructura del dataframe SAMPLE_SUBMISSION.\n",
    "original_sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_train.to_csv('../data/processed/original_train.csv',index=False)\n",
    "original_test.to_csv('../data/processed/original_test.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### =====================================================================\n",
    "### FIN.\n",
    "### ====================================================================="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
