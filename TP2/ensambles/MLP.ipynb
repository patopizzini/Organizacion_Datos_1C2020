{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "# Importacion de librerias necesarias\n",
    "import re, string, random, datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.python.keras import models\n",
    "from tensorflow.python.keras.layers import Dense\n",
    "from tensorflow.python.keras.layers import Dropout\n",
    "import tensorflow as tf\n",
    "\n",
    "# Importacion de librerias necesarias\n",
    "import re, string, random, datetime\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Split y K-Fold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Importacion de Keras\n",
    "# https://keras.io\n",
    "from keras.models import Model, Sequential, save_model, load_model\n",
    "from keras import layers\n",
    "from keras.layers import Embedding, Conv1D, Dropout, Input, GlobalMaxPooling1D, Dense, concatenate, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpia los datos, mediante el uso de expresiones regulares\n",
    "def f_remove_noise(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spell = SpellChecker()\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_text.append(spell.correction(word))\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)\n",
    "        \n",
    "def remove_punct(text):\n",
    "    table=str.maketrans('','',string.punctuation)\n",
    "    return text.translate(table)\n",
    "\n",
    "# Reference : https://gist.github.com/slowkow/7a7f61f495e3dbb7e3d767f97bd7304b\n",
    "def remove_emoji(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de entrada\n",
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (7613, 5)\n",
      "Shape test: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "# Carga de los archivos del set de datos\n",
    "# Set de train: carga\n",
    "df_twitter_train = pd.read_csv(train_path, sep=',')\n",
    "# Set de test: carga\n",
    "df_twitter_test = pd.read_csv(test_path, sep=',')\n",
    "\n",
    "# Print de los shapes\n",
    "print('Shape train: ' + str(df_twitter_train.shape))\n",
    "print('Shape test: ' + str(df_twitter_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_train['text'] = df_twitter_train['text'].apply(lambda x: f_remove_noise(x))\n",
    "df_twitter_test['text'] = df_twitter_test['text'].apply(lambda x: f_remove_noise(x))\n",
    "\n",
    "df_twitter_train['text'] = df_twitter_train['text'].apply(lambda x: correct_spellings(x))\n",
    "df_twitter_test['text'] = df_twitter_test['text'].apply(lambda x: correct_spellings(x))\n",
    "\n",
    "df_twitter_train['text'] = df_twitter_train['text'].apply(lambda x: remove_punct(x))\n",
    "df_twitter_test['text'] = df_twitter_test['text'].apply(lambda x: remove_punct(x))\n",
    "\n",
    "df_twitter_train['text'] = df_twitter_train['text'].apply(lambda x: remove_emoji(x))\n",
    "df_twitter_test['text'] = df_twitter_test['text'].apply(lambda x: remove_emoji(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keyword_stats = df_twitter_train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n",
    "keywords_disaster = keyword_stats.loc[keyword_stats['Disaster Probability']==1]\n",
    "keywords_no_disaster  = keyword_stats.loc[keyword_stats['Disaster Probability']==0]\n",
    "keyword_stats.sort_values('Disaster Probability', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, val_texts, train_labels , val_labels = train_test_split(\n",
    "    df_twitter_train['text'].values, df_twitter_train[\"target\"].values, test_size=0.10, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "                             min_df=2,      # only use words that appear at least X times\n",
    "                             #stop_words='english', # remove stop words\n",
    "                             #lowercase=True, # Convert everything to lower case \n",
    "                             use_idf=True,   # Use idf\n",
    "                             norm=u'l2',     # Normalization\n",
    "                             smooth_idf=True, # Prevents divide-by-zero errors\n",
    "                             ngram_range=(1,3),\n",
    "                             #dtype='int32',\n",
    "                             analyzer='word',\n",
    "                             strip_accents = 'unicode',\n",
    "                             decode_error = 'replace'\n",
    "                            )\n",
    "x_train = vectorizer.fit_transform(train_texts)\n",
    "x_val = vectorizer.transform(val_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selector = SelectKBest(f_classif, k=min(10000, x_train.shape[1]))\n",
    "selector.fit(x_train, train_labels)\n",
    "x_train = selector.transform(x_train)\n",
    "x_val = selector.transform(x_val)\n",
    "\n",
    "x_train = x_train.astype('float32')\n",
    "x_val = x_val.astype('float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model parameters\n",
    "learning_rate=1e-4\n",
    "epochs=1000\n",
    "batch_size=128\n",
    "layers=2\n",
    "units=64\n",
    "dropout_rate=0.2\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(Dropout(rate=dropout_rate, input_shape=x_train.shape[1:]))\n",
    "\n",
    "for _ in range(layers-1):\n",
    "    model.add(Dense(units=units, activation='relu'))\n",
    "    model.add(Dropout(rate=dropout_rate))\n",
    "\n",
    "model.add(Dense(units=1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = 'binary_crossentropy'\n",
    "optimizer = tf.keras.optimizers.Adam(lr=learning_rate)\n",
    "model.compile(optimizer=optimizer, loss=loss, metrics=['acc'])\n",
    "\n",
    "# Create callback for early stopping on validation loss. If the loss does\n",
    "# not decrease in two consecutive tries, stop training.\n",
    "callbacks = [tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)]\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "# Path de para guardar los modelos\n",
    "model_path = 'models/TP2.MLP.Iter.'\n",
    "num_folds = 20\n",
    "\n",
    "# Definimos los arrays para guardar los resultados\n",
    "# Train\n",
    "acc_per_fold_train = []\n",
    "loss_per_fold_train = []\n",
    "# Validacion\n",
    "acc_per_fold_validation = []\n",
    "loss_per_fold_validation = []\n",
    "\n",
    "# Definomos el K-fold Cross Validator a usar\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "X = x_train.toarray()\n",
    "y = train_labels\n",
    "\n",
    "\n",
    "# Hacemos el bucle para la cantidad de folds definidos\n",
    "fold_no = 1\n",
    "print('Total de folds: ', num_folds)\n",
    "for train, validation in kfold.split(X, y):\n",
    "    \n",
    "    # Imprimimos el progreso\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Entrenando, fold {fold_no}...')\n",
    "\n",
    "    # Train and validate model.\n",
    "    history = model.fit(X[train], y[train],\n",
    "            epochs=epochs,\n",
    "            callbacks=callbacks,\n",
    "            validation_data = (X[validation], y[validation]),\n",
    "            verbose=0,  # Logs once per epoch.\n",
    "            batch_size=batch_size)\n",
    "\n",
    "    # Print results.\n",
    "    history = history.history\n",
    "    print('Validation accuracy: {acc}, loss: {loss}'.format(acc=history['val_acc'][-1], loss=history['val_loss'][-1]))\n",
    "\n",
    "    # Generamos la metrica de entrenamiento\n",
    "    scores_train = model.evaluate(X[train], y[train], verbose=0)\n",
    "    print(f'Puntaje de entrenamiento para el fold {fold_no}: {model.metrics_names[0]} de {scores_train[0]}; {model.metrics_names[1]} de {scores_train[1]*100}%')\n",
    "    acc_per_fold_train.append(scores_train[1] * 100)\n",
    "    loss_per_fold_train.append(scores_train[0])\n",
    "\n",
    "    # Generamos la metrica de test\n",
    "    scores_validation = model.evaluate(X[validation], y[validation], verbose=0)\n",
    "    print(f'Puntaje de validación para el fold {fold_no}: {model.metrics_names[0]} de {scores_validation[0]}; {model.metrics_names[1]} de {scores_validation[1]*100}%')\n",
    "    acc_per_fold_validation.append(scores_validation[1] * 100)\n",
    "    loss_per_fold_validation.append(scores_validation[0])\n",
    "\n",
    "    # Guardamos el modelo\n",
    "    save_model(model, model_path + str(fold_no) + '.h5', save_format='h5')\n",
    "\n",
    "    # Plotteamos el resultado final\n",
    "    # plot_history(history)\n",
    "\n",
    "    # Incrementamos el fold\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "print('\\n')\n",
    "print('Puntaje promedio de entrenamiendo, para todos los folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_train)} (+- {np.std(acc_per_fold_train)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold_train)}')\n",
    "print('\\n')\n",
    "print('Puntaje promedio de validación, para todos los folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_validation)} (+- {np.std(acc_per_fold_validation)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold_validation)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df_twitter_train[\"target\"].values\n",
    "x_all = vectorizer.transform(df_twitter_train['text'].values)\n",
    "x_all = selector.transform(x_all)\n",
    "\n",
    "# Cargamos el modelo con el mejor puntaje\n",
    "filepath = model_path + str((acc_per_fold_validation.index(max(acc_per_fold_validation)))+1) + '.h5'\n",
    "loaded_model = load_model(filepath, custom_objects=None, compile=True)\n",
    "\n",
    "y_predict = loaded_model.predict_classes(x_all.toarray())\n",
    "\n",
    "score = f1_score(df_twitter_train[\"target\"].values, y_predict, average='weighted')\n",
    "print(\"*\"*50+\"\\n MLP Model f1_score: {:.5f}\\n\".format(score)+\"*\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_sample_submission = pd.read_csv('../data/sample_submission.csv')\n",
    "\n",
    "test_all = vectorizer.transform(df_twitter_test['text'].values)\n",
    "test_all = selector.transform(test_all)\n",
    "\n",
    "y_predict = loaded_model.predict_classes(test_all.toarray())\n",
    "y_predict[df_twitter_test.loc[df_twitter_test['keyword'].isin(list(keywords_disaster.index) )].index]=1\n",
    "y_predict[df_twitter_test.loc[df_twitter_test['keyword'].isin(list(keywords_no_disaster.index) )].index]=0\n",
    "\n",
    "# Path de salida para el submission\n",
    "submission_path = 'data/submits/submission.MLP.' + datetime.datetime.now().isoformat() + '.csv'\n",
    "submission_path = submission_path.replace('-','.').replace(':','.')\n",
    "\n",
    "original_sample_submission[\"target\"] = y_predict\n",
    "original_sample_submission.to_csv(submission_path, index=False)\n",
    "original_sample_submission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SUBMIT\n",
    "print(submission_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MODELO\n",
    "print(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNN = pd.read_csv('data/submits/submission.2020.08.03T21.22.36.100631.csv')\n",
    "# XGBOOST = pd.read_csv('data/submits/submission_XGB_12.csv')\n",
    "# MLP = pd.read_csv('data/submits/submission_MLP_08.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.8109102053325161\n",
    "# ediccion =(CNN[\"target\"]+XGBOOST[\"target\"]+MLP[\"target\"])/3\n",
    "# y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.8026356114005516\n",
    "# ediccion =(CNN[\"target\"]+MLP[\"target\"])/2\n",
    "# y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.7716825007661661\n",
    "# ediccion =(XGBOOST[\"target\"]+MLP[\"target\"])/2\n",
    "# y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ENSAMBLE CON: 0.7805700275819798\n",
    "# ediccion =(CNN[\"target\"]+XGBOOST[\"target\"])/2\n",
    "# y_pred_ENS = np.where(ediccion>0.5, 1, 0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
