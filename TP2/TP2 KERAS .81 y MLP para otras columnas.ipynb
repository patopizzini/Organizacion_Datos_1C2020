{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75.06 - 1C202 - TP2 \n",
    "# Competencia de Machine learning\n",
    "#\n",
    "# https://www.kaggle.com/c/nlp-getting-started/overview\n",
    "#\n",
    "# Fuentes:\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "# https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n",
    "\n",
    "# Importacion de librerias necesarias\n",
    "import re, string, random, datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split y K-Fold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Importacion de Keras\n",
    "# https://keras.io\n",
    "from keras.models import Model, Sequential, save_model, load_model\n",
    "from keras.layers import Embedding, Conv1D, Dropout, Input, GlobalMaxPooling1D, Dense, concatenate, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "# Importaci√≥n para MLP.\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D\n",
    "from tensorflow.keras.layers import MaxPooling2D\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import Flatten\n",
    "from tensorflow.keras.layers import Input\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import glob\n",
    "#import cv2\n",
    "import os\n",
    "\n",
    "\n",
    "#Configuracion general\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpia los datos, mediante el uso de expresiones regulares\n",
    "def f_remove_noise(text):\n",
    "    text = re.sub('\\[.*?\\]', '', text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub('<.*?>+', '', text)\n",
    "    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = re.sub('\\w*\\d\\w*', '', text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]+','', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_twitter_train['text'] = df_twitter_train['text'].apply(lambda x: f_remove_noise(x))\n",
    "df_twitter_test['text'] = df_twitter_test['text'].apply(lambda x: f_remove_noise(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (7613, 5)\n",
      "Shape test: (3263, 4)\n"
     ]
    }
   ],
   "source": [
    "# Path de entrada\n",
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'\n",
    "\n",
    "# Carga de los archivos del set de datos\n",
    "# Set de train: carga\n",
    "df_twitter_train = pd.read_csv(train_path, sep=',')\n",
    "# Set de test: carga\n",
    "df_twitter_test = pd.read_csv(test_path, sep=',')\n",
    "\n",
    "# Print de los shapes\n",
    "print('Shape train: ' + str(df_twitter_train.shape))\n",
    "print('Shape test: ' + str(df_twitter_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Count</th>\n",
       "      <th>Disaster Probability</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>keyword</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>debris</th>\n",
       "      <td>37</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wreckage</th>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>derailment</th>\n",
       "      <td>39</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>outbreak</th>\n",
       "      <td>40</td>\n",
       "      <td>0.975000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oil%20spill</th>\n",
       "      <td>38</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>typhoon</th>\n",
       "      <td>38</td>\n",
       "      <td>0.973684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide%20bombing</th>\n",
       "      <td>33</td>\n",
       "      <td>0.969697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>suicide%20bomber</th>\n",
       "      <td>31</td>\n",
       "      <td>0.967742</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bombing</th>\n",
       "      <td>29</td>\n",
       "      <td>0.931034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rescuers</th>\n",
       "      <td>35</td>\n",
       "      <td>0.914286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Count  Disaster Probability\n",
       "keyword                                       \n",
       "debris                37              1.000000\n",
       "wreckage              39              1.000000\n",
       "derailment            39              1.000000\n",
       "outbreak              40              0.975000\n",
       "oil%20spill           38              0.973684\n",
       "typhoon               38              0.973684\n",
       "suicide%20bombing     33              0.969697\n",
       "suicide%20bomber      31              0.967742\n",
       "bombing               29              0.931034\n",
       "rescuers              35              0.914286"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keyword_stats = df_twitter_train.groupby('keyword').agg({'text':np.size, 'target':np.mean}).rename(columns={'text':'Count', 'target':'Disaster Probability'})\n",
    "keywords_disaster = keyword_stats.loc[keyword_stats['Disaster Probability']==1]\n",
    "keywords_no_disaster  = keyword_stats.loc[keyword_stats['Disaster Probability']==0]\n",
    "keyword_stats.sort_values('Disaster Probability', ascending=False).head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (7613, 4)\n",
      "Shape test: (3263, 3)\n"
     ]
    }
   ],
   "source": [
    "# Path de entrada    \n",
    "train_path = 'data/processed/train.2020.08.04T11.51.15.683605.csv' #Cambiar por el archivo pre-procesado a usar!\n",
    "test_path = 'data/processed/test.2020.08.04T11.51.15.683605.csv' #Cambiar por el archivo pre-procesado a usar!\n",
    "\n",
    "# Configuracion\n",
    "maxlen = 100\n",
    "\n",
    "# Carga de los archivos del set de datos\n",
    "# Set de train: carga\n",
    "df_twitter_train = pd.read_csv(train_path, sep=',')\n",
    "# Set de test: carga\n",
    "df_twitter_test = pd.read_csv(test_path, sep=',')\n",
    "\n",
    "# Print de los shapes\n",
    "print('Shape train: ' + str(df_twitter_train.shape))\n",
    "print('Shape test: ' + str(df_twitter_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 7613 entries, 0 to 7612\n",
      "Data columns (total 4 columns):\n",
      " #   Column   Non-Null Count  Dtype \n",
      "---  ------   --------------  ----- \n",
      " 0   id       7613 non-null   int64 \n",
      " 1   keyword  7552 non-null   object\n",
      " 2   text     7613 non-null   object\n",
      " 3   target   7613 non-null   int64 \n",
      "dtypes: int64(2), object(2)\n",
      "memory usage: 238.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_twitter_train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con X e y para entrenar\n",
    "tweets = df_twitter_train['text'].values\n",
    "y = df_twitter_train['target'].values\n",
    "\n",
    "# Separamos X para el set de test\n",
    "tweets_predict = df_twitter_test['text'].values\n",
    "\n",
    "# Tokenizamos los textos\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# Convertimos a secuencias\n",
    "X = tokenizer.texts_to_sequences(tweets)\n",
    "X_predict = tokenizer.texts_to_sequences(tweets_predict)\n",
    "\n",
    "# Paddeamos a maxlen\n",
    "X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "X_predict = pad_sequences(X_predict, padding='post', maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 100)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 100, 300)     4953900     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 99, 100)      60100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 98, 100)      90100       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 97, 100)      120100      embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d (GlobalMax (None, 100)          0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 100)          0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_2 (GlobalM (None, 100)          0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 300)          0           global_max_pooling1d[0][0]       \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "                                                                 global_max_pooling1d_2[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 256)          77056       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 256)          0           dense[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            257         dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation (Activation)         (None, 1)            0           dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 5,301,513\n",
      "Trainable params: 5,301,513\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Path de salida para el submission\n",
    "submission_path = 'data/submits/submission.' + datetime.datetime.now().isoformat() + '.csv'\n",
    "submission_path = submission_path.replace('-','.').replace(':','.')\n",
    "\n",
    "# Cargamos el modelo con el mejor puntaje\n",
    "filepath = 'models.backup.81/' + 'TP2.Keras.Conv1D.Iter.2' + '.h5'\n",
    "loaded_model = load_model(filepath, custom_objects=None, compile=True)\n",
    "\n",
    "# Mostramos el detalle del modelo\n",
    "loaded_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operaci√≥n finalizada!\n",
      "\n",
      "Generado submit: 'data/submits/submission.2020.08.04T11.52.05.274146.csv' - (3263) registros.\n"
     ]
    }
   ],
   "source": [
    "# Prediccion en base al modelo entrenado\n",
    "y_predict = (loaded_model.predict(X_predict) > 0.5).astype('int32')\n",
    "y_predict[df_twitter_test.loc[df_twitter_test['keyword'].isin(list(keywords_disaster.index) )].index]=1\n",
    "y_predict[df_twitter_test.loc[df_twitter_test['keyword'].isin(list(keywords_no_disaster.index) )].index]=0\n",
    "\n",
    "# Generacion del dataframe que generara el submit para la competencia\n",
    "kaggle_submission = pd.DataFrame(df_twitter_test,columns = ['id'])\n",
    "kaggle_submission['target'] = y_predict\n",
    "\n",
    "kaggle_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "# Imprimimos un resumen de la operacion\n",
    "print('Operaci√≥n finalizada!\\n')\n",
    "print('Generado submit: \\'' + submission_path + '\\' - (' + str(len(kaggle_submission['target'].index)) + ') registros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
