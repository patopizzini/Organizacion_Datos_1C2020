{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 75.06 - 1C202 - TP2 \n",
    "# Competencia de Machine learning\n",
    "#\n",
    "# https://www.kaggle.com/c/nlp-getting-started/overview\n",
    "#\n",
    "# Fuentes:\n",
    "# https://realpython.com/python-keras-text-classification/\n",
    "# https://towardsdatascience.com/data-augmentation-in-nlp-2801a34dfc28\n",
    "\n",
    "# Importacion de librerias necesarias\n",
    "import re, string, random, datetime\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Split y K-Fold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "# Importacion de Keras\n",
    "# https://keras.io\n",
    "from keras.models import Model, Sequential, save_model, load_model\n",
    "from keras.layers import Embedding, Conv1D, Dropout, Input, GlobalMaxPooling1D, Dense, concatenate, Activation\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#Configuracion general\n",
    "plt.style.use('ggplot')\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seccion de CONFIGURACION\n",
    "\n",
    "#Configuración de la limpieza de datos\n",
    "#Pasaje a minúsculas:   True\n",
    "#Limpieza básica:       True\n",
    "#Agregado de keywords:  False\n",
    "#Remoción de stopwords: False\n",
    "#Lematización:          False\n",
    "#Aumento del set:       False\n",
    "#Chequeo final:         True\n",
    "\n",
    "# Path de entrada\n",
    "train_path = 'data/processed/train.2020.08.03T11.00.50.547958.csv' #Cambiar por el archivo pre-procesado a usar!\n",
    "test_path = 'data/processed/test.2020.08.03T11.00.50.547958.csv' #Cambiar por el archivo pre-procesado a usar!\n",
    "\n",
    "# Configuracion del modelo\n",
    "maxlen = 100\n",
    "embedding_dim = 300\n",
    "embeddings_path = 'embeddings/glove.42B.300d.txt'\n",
    "# Embeddings disponibles en:\n",
    "# https://www.kaggle.com/yutanakamura/glove42b300dtxt\n",
    "\n",
    "# Configuracion de entrenamiento\n",
    "epochs = 15\n",
    "verbose = False\n",
    "batch_size = 16\n",
    "\n",
    "# Canditad de folds para K-Fold Cross-Validation\n",
    "num_folds = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciones AUXILIARES\n",
    "# Grafica para un modelo los resultados del entrenamiento\n",
    "def plot_history(history):\n",
    "    acc = history.history['accuracy']\n",
    "    val_acc = history.history['val_accuracy']\n",
    "    loss = history.history['loss']\n",
    "    val_loss = history.history['val_loss']\n",
    "    x = range(1, len(acc) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(x, acc, 'b', label='Entrenamiento')\n",
    "    plt.plot(x, val_acc, 'r', label='Validación')\n",
    "    plt.title('Accuracy: validación y entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(x, loss, 'b', label='Entrenamiento')\n",
    "    plt.plot(x, val_loss, 'r', label='Validación')\n",
    "    plt.title('Loss: validación y entrenamiento')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "# Genera la matriz de embeddings\n",
    "def create_embedding_matrix(filepath, word_index, embedding_dim):\n",
    "    vocab_size = len(word_index) + 1  # 0 es un indice reservado, sumamos 1\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "    with open(filepath, encoding='utf-8') as f:\n",
    "        for line in f:\n",
    "            word, *vector = line.split()\n",
    "            if word in word_index:\n",
    "                idx = word_index[word] \n",
    "                embedding_matrix[idx] = np.array(\n",
    "                    vector, dtype=np.float32)[:embedding_dim]\n",
    "\n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape train: (7613, 4)\n",
      "Shape test: (3263, 3)\n"
     ]
    }
   ],
   "source": [
    "# Carga de los archivos del set de datos\n",
    "# Set de train: carga\n",
    "df_twitter_train = pd.read_csv(train_path, sep=',')\n",
    "# Set de test: carga\n",
    "df_twitter_test = pd.read_csv(test_path, sep=',')\n",
    "\n",
    "# Print de los shapes\n",
    "print('Shape train: ' + str(df_twitter_train.shape))\n",
    "print('Shape test: ' + str(df_twitter_test.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nos quedamos con X e y para entrenar\n",
    "tweets = df_twitter_train['text'].values\n",
    "y = df_twitter_train['target'].values\n",
    "\n",
    "# Separamos X para el set de test\n",
    "tweets_predict = df_twitter_test['text'].values\n",
    "\n",
    "# Tokenizamos los textos\n",
    "tokenizer = Tokenizer(num_words=5000)\n",
    "tokenizer.fit_on_texts(tweets)\n",
    "\n",
    "# Convertimos a secuencias\n",
    "X = tokenizer.texts_to_sequences(tweets)\n",
    "X_predict = tokenizer.texts_to_sequences(tweets_predict)\n",
    "\n",
    "# Calculamos el tamaño\n",
    "vocab_size = len(tokenizer.word_index) + 1  # 0 es un indice reservado, sumamos 1\n",
    "\n",
    "# Paddeamos a maxlen\n",
    "X = pad_sequences(X, padding='post', maxlen=maxlen)\n",
    "X_predict = pad_sequences(X_predict, padding='post', maxlen=maxlen)\n",
    "\n",
    "# Generacion de la matriz de embeddings\n",
    "embedding_matrix = create_embedding_matrix(embeddings_path,tokenizer.word_index, embedding_dim)\n",
    "\n",
    "# Vemos que cobertura tenemos con los embeddings utilizados\n",
    "nonzero_elements = np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1))\n",
    "print('Cobertura de vocabulario: ' + str(nonzero_elements / vocab_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definimos y entrenamos el modelo\n",
    "# Define el modelo a utilizar, con Conv1D\n",
    "tweet_input = Input(shape=(maxlen,), dtype='int32')\n",
    "\n",
    "tweet_encoder = Embedding(vocab_size, embedding_dim, weights=[embedding_matrix], input_length=maxlen, trainable=True)(tweet_input)\n",
    "bigram_branch = Conv1D(filters=maxlen, kernel_size=2, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "bigram_branch = GlobalMaxPooling1D()(bigram_branch)\n",
    "trigram_branch = Conv1D(filters=maxlen, kernel_size=3, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "trigram_branch = GlobalMaxPooling1D()(trigram_branch)\n",
    "fourgram_branch = Conv1D(filters=maxlen, kernel_size=4, padding='valid', activation='relu', strides=1)(tweet_encoder)\n",
    "fourgram_branch = GlobalMaxPooling1D()(fourgram_branch)\n",
    "merged = concatenate([bigram_branch, trigram_branch, fourgram_branch], axis=1)\n",
    "\n",
    "merged = Dense(256, activation='relu')(merged)\n",
    "merged = Dropout(0.2)(merged)\n",
    "merged = Dense(1)(merged)\n",
    "output = Activation('sigmoid')(merged)\n",
    "\n",
    "# K-Fold Cross Validation\n",
    "# Path de para guardar los modelos\n",
    "model_path = 'models/TP2.Keras.Conv1D.Iter.'\n",
    "\n",
    "# Definimos los arrays para guardar los resultados\n",
    "# Train\n",
    "acc_per_fold_train = []\n",
    "loss_per_fold_train = []\n",
    "# Validacion\n",
    "acc_per_fold_validation = []\n",
    "loss_per_fold_validation = []\n",
    "\n",
    "# Definomos el K-fold Cross Validator a usar\n",
    "kfold = KFold(n_splits=num_folds, shuffle=True)\n",
    "\n",
    "# Hacemos el bucle para la cantidad de folds definidos\n",
    "fold_no = 1\n",
    "print('Total de folds: ', num_folds)\n",
    "for train, validation in kfold.split(X, y):\n",
    "\n",
    "    # Imprimimos el progreso\n",
    "    print('------------------------------------------------------------------------')\n",
    "    print(f'Entrenando, fold {fold_no}...')\n",
    "\n",
    "    # Definimos la arquitectura del modelo\n",
    "    model = Model(inputs=[tweet_input], outputs=[output])\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    # Hacemos el fit\n",
    "    history = model.fit(X[train], y[train],\n",
    "                        epochs = epochs,\n",
    "                        verbose = verbose,\n",
    "                        validation_data = (X[validation], y[validation]),\n",
    "                        batch_size = batch_size)      \n",
    "\n",
    "    # Generamos la metrica de entrenamiento\n",
    "    scores_train = model.evaluate(X[train], y[train], verbose=0)\n",
    "    print(f'Puntaje de entrenamiento para el fold {fold_no}: {model.metrics_names[0]} de {scores_train[0]}; {model.metrics_names[1]} de {scores_train[1]*100}%')\n",
    "    acc_per_fold_train.append(scores_train[1] * 100)\n",
    "    loss_per_fold_train.append(scores_train[0])\n",
    "\n",
    "    # Generamos la metrica de test\n",
    "    scores_validation = model.evaluate(X[validation], y[validation], verbose=0)\n",
    "    print(f'Puntaje de validación para el fold {fold_no}: {model.metrics_names[0]} de {scores_validation[0]}; {model.metrics_names[1]} de {scores_validation[1]*100}%')\n",
    "    acc_per_fold_validation.append(scores_validation[1] * 100)\n",
    "    loss_per_fold_validation.append(scores_validation[0])\n",
    "\n",
    "    # Guardamos el modelo\n",
    "    save_model(model, model_path + str(fold_no) + '.h5', save_format='h5')\n",
    "\n",
    "    # Plotteamos el resultado final\n",
    "    plot_history(history)\n",
    "\n",
    "    # Incrementamos el fold\n",
    "    fold_no = fold_no + 1\n",
    "    \n",
    "print('\\n')\n",
    "print('Puntaje promedio de entrenamiendo, para todos los folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_train)} (+- {np.std(acc_per_fold_train)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold_train)}')\n",
    "print('\\n')\n",
    "print('Puntaje promedio de validación, para todos los folds:')\n",
    "print(f'> Accuracy: {np.mean(acc_per_fold_validation)} (+- {np.std(acc_per_fold_validation)})')\n",
    "print(f'> Loss: {np.mean(loss_per_fold_validation)}')\n",
    "print('------------------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de salida para el submission\n",
    "submission_path = 'data/submits/submission.' + datetime.datetime.now().isoformat() + '.csv'\n",
    "submission_path = submission_path.replace('-','.').replace(':','.')\n",
    "\n",
    "# Cargamos el modelo con el mejor puntaje\n",
    "filepath = model_path + str((acc_per_fold_validation.index(max(acc_per_fold_validation)))+1) + '.h5'\n",
    "loaded_model = load_model(filepath, custom_objects=None, compile=True)\n",
    "\n",
    "# Prediccion en base al modelo entrenado\n",
    "y_predict = (loaded_model.predict(X_predict) > 0.5).astype('int32')\n",
    "\n",
    "# Generacion del dataframe que generara el submit para la competencia\n",
    "kaggle_submission = pd.DataFrame(df_twitter_test,columns = ['id'])\n",
    "kaggle_submission['target'] = y_predict\n",
    "\n",
    "kaggle_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "# Imprimimos un resumen de la operacion\n",
    "print('Operación finalizada!\\n')\n",
    "print('Generado submit: \\'' + submission_path + '\\' - (' + str(len(kaggle_submission['target'].index)) + ') registros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lo repetimos con todo el dataset\n",
    "# Creamos y entrenamos nuevamente\n",
    "model2 = create_model(num_filters = num_filters,\n",
    "                         kernel_size = kernel_size,\n",
    "                         vocab_size = vocab_size,\n",
    "                         embedding_dim = embedding_dim,\n",
    "                         maxlen = maxlen)\n",
    "\n",
    "# Hacemos el fit\n",
    "history = model2.fit(X, y,\n",
    "                        epochs = epochs,\n",
    "                        verbose = verbose,\n",
    "                        batch_size = batch_size)\n",
    "\n",
    "# Prediccion en base al modelo entrenado\n",
    "y_predict = (model2.predict(X_predict) > 0.5).astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path de salida para el submission\n",
    "submission_path = 'data/submits/submission.' + datetime.datetime.now().isoformat() + '.csv'\n",
    "submission_path = submission_path.replace('-','.').replace(':','.')\n",
    "\n",
    "# Generacion del dataframe que generara el submit para la competencia\n",
    "kaggle_submission = pd.DataFrame(df_twitter_test,columns = ['id'])\n",
    "kaggle_submission['target'] = y_predict\n",
    "\n",
    "kaggle_submission.to_csv(submission_path, index=False)\n",
    "\n",
    "# Imprimimos un resumen de la operacion\n",
    "print('Operación finalizada!\\n')\n",
    "print('Generado submit: \\'' + submission_path + '\\' - (' + str(len(kaggle_submission['target'].index)) + ') registros.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
